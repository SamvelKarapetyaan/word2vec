{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdJDUBpKPGbp"
      },
      "source": [
        "# Word2Vec implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMgim1D3PGbr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "from multiset import Multiset # Use *pip install multiset*\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwFQhBH6PGbr"
      },
      "source": [
        "# Making data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-7X-DrmPGbs",
        "outputId": "775155da-63d9-4c61-fdb4-d39280316a30"
      },
      "outputs": [],
      "source": [
        "data = open(\"corpus_100k\", \"r\").read()\n",
        "\n",
        "def lower(txt):\n",
        "  return txt.lower()\n",
        "\n",
        "data_split = list(map(lower, data.split()))\n",
        "\n",
        "data_len = len(data_split)\n",
        "\n",
        "words_counts = Multiset(data_split) # *word*: *count*\n",
        "uniq_words = Multiset(set(data_split)) # *word*: 1\n",
        "\n",
        "words_counts = words_counts - uniq_words * 10000 # words used at least 100 times\n",
        "\n",
        "counts_lst = words_counts.values()\n",
        "\n",
        "prob_lst = list(np.array(list(counts_lst)) / sum(counts_lst))\n",
        "\n",
        "vocab_words = list(set(words_counts))\n",
        "vocab_len = len(vocab_words)\n",
        "\n",
        "print(f\"From {data_len} words we got {vocab_len} to vocabulary.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "l3U0NfZdPGbs",
        "outputId": "e0e1b569-d0fe-41af-ef56-464835aaf36a"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "ohe.fit(np.array(vocab_words).reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "tZ7ejRBpBce4",
        "outputId": "496f0caf-5ebf-4964-9859-82eaefd9c3fc"
      },
      "outputs": [],
      "source": [
        "# precollect some pairs for speed\n",
        "\n",
        "indices = []\n",
        "\n",
        "for i in tqdm(range(0, data_len, 2)):\n",
        "  if data_split[i] in ohe.categories_[0] and data_split[i + 1] in ohe.categories_[0]:\n",
        "    indices.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkxsF04gcON1"
      },
      "outputs": [],
      "source": [
        "def get_sample():\n",
        "  i = random.choice(indices)\n",
        "\n",
        "  x = ohe.transform(np.array(data_split[i]).reshape(-1, 1))\n",
        "  y = ohe.transform(np.array(data_split[i + 1]).reshape(-1, 1))\n",
        "\n",
        "  k = random.choices(vocab_words, k=10, weights=prob_lst) # Use random.choices() with weights\n",
        "  k = ohe.transform(np.array(k).reshape(-1, 1))\n",
        "\n",
        "  return x, y, k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4C7X-P3cjQh",
        "outputId": "05411ef3-61b1-4ffd-b8d9-6158424a13e2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def neg_sampling_loss(u_o, v_c, u_k):\n",
        "  return -tf.math.log(tf.math.sigmoid(u_o @ tf.transpose(v_c))) - tf.reduce_mean(tf.math.log(tf.math.sigmoid(- u_k @ tf.transpose(v_c))))\n",
        "\n",
        "# Network\n",
        "input_encoder = L.Input((vocab_len,), name=\"input\")\n",
        "encoder_layer = L.Dense(200, name=\"encoder\", use_bias=False)(input_encoder)\n",
        "decoder_layer = L.Dense(vocab_len, name=\"decoder\", use_bias=False)(encoder_layer)\n",
        "\n",
        "network = tf.keras.Model(input_encoder, decoder_layer)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Steps\n",
        "steps = 10000\n",
        "\n",
        "global_loss = 0\n",
        "for i in tqdm(range(steps)):\n",
        "  x, y, k = get_sample()\n",
        "\n",
        "  x, y, k = tf.constant(x, dtype=tf.float32), tf.constant(y, dtype=tf.float32), tf.constant(k, dtype=tf.float32)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      u_o = network(x)\n",
        "      u_k = network(k)\n",
        "\n",
        "      loss = neg_sampling_loss(u_o, y, u_k)\n",
        "\n",
        "  # Collect trainable variables\n",
        "  train_vars = network.trainable_variables\n",
        "\n",
        "  # Calculate gradients\n",
        "  grad = tape.gradient(loss, train_vars)\n",
        "\n",
        "  # Apply gradients\n",
        "  optimizer.apply_gradients(zip(grad, train_vars))\n",
        "\n",
        "  global_loss += loss[0][0]\n",
        "  if i % 500 == 0:\n",
        "    tf.print(\"LOSS: \", global_loss / 500)\n",
        "    global_loss = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj4eIWwSwCMM"
      },
      "outputs": [],
      "source": [
        "network.save_weights(\"network/network.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd3r7_Q4Rm6S"
      },
      "outputs": [],
      "source": [
        "embeddings_np = network.weights[0]\n",
        "transformed = PCA(n_components=2).fit_transform(embeddings_np)\n",
        "\n",
        "x_1, x_2 = transformed[:, 0], transformed[:, 1]\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=x_1, y=x_2, mode=\"markers+text\", text=list(vocab_words)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
